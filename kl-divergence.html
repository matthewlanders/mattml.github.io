<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Kullback–Leibler (KL) Divergence</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Matthew Landers</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="industry-experience.html">Experience</a></div>
<div class="menu-item"><a href="static/Matthew_Landers_CV.pdf" target="blank">CV</a></div>
<div class="menu-category">Notes</div>
<div class="menu-item"><a href="about.html"><i>About&nbsp;these&nbsp;notes</i></a></div>
<div class="menu-item"><a href="glossary.html">Glossary</a></div>
<div class="menu-item"><a href="pseudocode.html">Pseudocode</a></div>
<div class="menu-item"><a href="about-rl.html">About&nbsp;RL</a></div>
<div class="menu-item"><a href="mdp.html">MDPs</a></div>
<div class="menu-item"><a href="value-functions-and-policies.html">Value&nbsp;Func.&nbsp;&amp;&nbsp;Policies</a></div>
<div class="menu-item"><a href="dynamic-programming-for-mdps.html">DP&nbsp;for&nbsp;MDPs</a></div>
<div class="menu-item"><a href="policy-and-value-iteration-proofs.html">DP&nbsp;for&nbsp;MDPs&nbsp;Proofs</a></div>
<div class="menu-item"><a href="model-free-prediction.html">Model-Free&nbsp;Prediction</a></div>
<div class="menu-item"><a href="prediction-with-function-approximation.html">Prediction&nbsp;with&nbsp;Approx.</a></div>
<div class="menu-item"><a href="model-free-control.html">Model-Free&nbsp;Control</a></div>
<div class="menu-item"><a href="on-policy-control-with-function-approximation.html">On-Policy&nbsp;Control<br /> with&nbsp;Approximation</a></div>
<div class="menu-item"><a href="off-policy-control-with-function-approximation.html">Off-Policy&nbsp;Control<br /> with&nbsp;Approximation</a></div>
<div class="menu-item"><a href="importance-sampling.html">Importance&nbsp;Sampling</a></div>
<div class="menu-item"><a href="learnability-of-rl-objectives.html">Learnability&nbsp;of<br /> RL&nbsp;Objectives</a></div>
<div class="menu-item"><a href="the-deadly-triad.html">The&nbsp;Deadly&nbsp;Triad</a></div>
<div class="menu-item"><a href="deep-q-learning.html">Deep&nbsp;Q-Learning</a></div>
<div class="menu-item"><a href="policy-gradients.html">Policy&nbsp;Gradients</a></div>
<div class="menu-item"><a href="actor-critic.html">Actor-Critic&nbsp;Framework</a></div>
<div class="menu-item"><a href="ddpg.html">DPG&nbsp;&amp;&nbsp;DDPG</a></div>
<div class="menu-item"><a href="reparameterization-trick.html">Reparameterization&nbsp;Trick</a></div>
<div class="menu-item"><a href="sac.html">Soft&nbsp;Actor-Critic</a></div>
<div class="menu-item"><a href="grpo.html">Group&nbsp;Relative&nbsp;Policy<br /> Optimization</a></div>
<div class="menu-item"><a href="transformer.html">Transformer</a></div>
<div class="menu-item"><a href="trpo.html">TRPO</a></div>
<div class="menu-item"><a href="conjugate-gradient-method.html">Conjugate&nbsp;Gradient&nbsp;Method</a></div>
<div class="menu-item"><a href="ppo.html">PPO</a></div>
<div class="menu-item"><a href="double-q-learning.html">Double&nbsp;Q-Learning</a></div>
<div class="menu-item"><a href="td3.html">TD3</a></div>
<div class="menu-item"><a href="nn-verification.html">NN&nbsp;Verification</a></div>
<div class="menu-item"><a href="drl-verification.html">DRL&nbsp;Verification</a></div>
<div class="menu-item"><a href="alphazero.html">AlphaZero&nbsp;(chess)</a></div>
<div class="menu-item"><a href="bayes-theorem-for-probability-distributions.html">Bayes&rsquo;&nbsp;for&nbsp;Distributions</a></div>
<div class="menu-item"><a href="backpropagation.html">Backpropagation</a></div>
<div class="menu-item"><a href="off-policy-evaluation.html">Off-Policy&nbsp;Evaluation</a></div>
<div class="menu-item"><a href="constrained-mdps.html">Constrained&nbsp;MDPs</a></div>
<div class="menu-item"><a href="cpo.html">Constrained&nbsp;Policy&nbsp;<br />Optimization</a></div>
<div class="menu-item"><a href="pid-lagrangian.html">PID&nbsp;Lagrangian</a></div>
<div class="menu-item"><a href="successor-features.html">Successor&nbsp;Features</a></div>
<div class="menu-item"><a href="policy-distillation.html">Policy&nbsp;Distillation</a></div>
<div class="menu-item"><a href="kl-divergence.html" class="current">KL&nbsp;Divergence</a></div>
<div class="menu-item"><a href="implicit-q-learning.html">Implicit&nbsp;Q-Learning</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Kullback–Leibler (KL) Divergence</h1>
</div>
<p><i>Revised September 2, 2025</i>
</p>
<p><i>The <a href="importance-sampling.html" target=&ldquo;blank&rdquo;>Importance Sampling note</a> is optional but recommended background reading.</i>
</p>
<p><a href="importance-sampling.html" target=&ldquo;blank&rdquo;>Importance sampling</a> expresses the return of a target policy \(\pi\) using trajectories generated by a behavior policy \(\mu\):
</p>
<p style="text-align:center">
\[
\begin{equation*}
    \mathbb{E}_{\tau \sim p_\pi}[G(\tau)] = \mathbb{E}_{\tau \sim p_\mu}\!\left[\frac{p_\pi(\tau)}{p_\mu(\tau)}\,G(\tau)\right].
\end{equation*}
\]
</p><p>where \(\tau=(s_0,a_0,\dots,s_{T-1},s_T)\) and \(p_\pi(\tau)=\rho_0(s_0)\prod_{t=0}^{T-1}\pi(a_t\mid s_t)\,P(s_{t+1}\mid s_t,a_t)\) with the same \(\rho_0\) and \(P\) for \(\pi\) and \(\mu\).
</p>
<p>The importance weight for a trajectory factorizes into a product of per-step action probabilities:
</p>
<p style="text-align:center">
\[
\begin{equation*}
    \frac{p_\pi(\tau)}{p_\mu(\tau)}
    = \prod_{t=0}^{T-1} \frac{\pi(a_t\mid s_t)}{\mu(a_t\mid s_t)}.
\end{equation*}
\]
</p><p>This product has high variance when \(\pi\) and \(\mu\) differ substantially. Moreover, its multiplicative structure is numerically unstable and obscures the contribution of individual decisions. Applying a logarithm converts products into sums, yielding a per-step additive form:
</p>
<p style="text-align:center">
\[
\begin{equation*}
    \log \frac{p_\pi(\tau)}{p_\mu(\tau)}
    = \sum_{t=0}^{T-1} \log \frac{\pi(a_t\mid s_t)}{\mu(a_t\mid s_t)} \;.
\end{equation*}
\]
</p><p>This decomposition is more stable and isolates the influence of each action on the total ratio. Averaging this quantity under \(p_\pi\) produces:
</p>
<p style="text-align:center">
\[
\begin{equation*}
    \mathbb{E}_{\tau \sim p_\pi}\!\left[ \log \frac{p_\pi(\tau)}{p_\mu(\tau)} \right],
\end{equation*}
\]
</p><p>which defines the <i>Kullback–Leibler (KL) divergence</i>:
</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:kl-divergence}
    D_{KL}(p_\pi \,\Vert\, p_\mu)
    = \mathbb{E}_{\tau \sim p_\pi}\!\left[ \log \frac{p_\pi(\tau)}{p_\mu(\tau)} \right] \;.
\end{equation}
\]
</p><p>Importance sampling fails when \(\pi(a\mid s)&gt;0\) but \(\mu(a\mid s)=0\), since the ratio \(\tfrac{\pi(a\mid s)}{\mu(a\mid s)}\) is undefined. More generally, stable estimates require the per-step ratios \(\tfrac{\pi(\cdot\mid s)}{\mu(\cdot\mid s)}\) to remain near one on the support of \(d_\pi\), the discounted state visitation distribution under \(\pi\). This condition corresponds to a small KL divergence between \(p_\pi\) and \(p_\mu\). <a href="trpo.html" target=&ldquo;blank&rdquo;>TRPO</a> constrains the expected per-state KL between the new and old policy under the old state distribution; <a href="ppo.html" target=&ldquo;blank&rdquo;>PPO</a> achieves a similar effect via clipping.
</p>
<p>In maximum-entropy reinforcement learning such as <a href="sac.html" target=&ldquo;blank&rdquo;>SAC</a>, KL divergence acts as a projection onto \(P_{\text{opt}}\), the distribution over optimal trajectories. Minimizing \(D_{KL}(p_\pi \,\Vert\, P_{\text{opt}})\) shifts probability mass toward high-return trajectories:
</p>
<p style="text-align:center">
\[
\begin{align}
    D_{KL}(p_\pi \Vert P_{\text{opt}})
    &amp;= \mathbb{E}_{\tau \sim p_\pi}\!\left[\log p_\pi(\tau) - \log P_{\text{opt}}(\tau)\right] &amp;&amp; \text{by Equation } \eqref{eq:kl-divergence} \label{eq:entropy-kl} \\

    &amp;= \mathbb{E}_{\tau \sim p_\pi}[\log p_\pi(\tau)] - \mathbb{E}_{\tau \sim p_\pi} \left[ \tfrac{1}{\alpha}\sum_t \gamma^t r(s_t,a_t) - \log Z \right] &amp;&amp; \text{with } P_{\text{opt}}(\tau)=\tfrac{1}{Z}\exp\!\Big(\tfrac{1}{\alpha}\sum_t \gamma^t r(s_t,a_t)\Big) \nonumber \\

    &amp;= -H(p_\pi) - \tfrac{1}{\alpha}\mathbb{E}_{\tau \sim p_\pi}\!\Big[\sum_t \gamma^t r(s_t,a_t)\Big] + \log Z &amp;&amp; \text{where } H(p_\pi) = -\,\mathbb{E}_{\tau \sim p_\pi}[\log p_\pi(\tau)] \nonumber
\end{align}
\]
</p><p>Since \(\log Z\) and \(\alpha\) are constant with respect to \(\pi\), minimizing Equation \(\eqref{eq:entropy-kl}\) is equivalent to maximizing:
</p>
<p style="text-align:center">
\[
\begin{equation*}
    \mathbb{E}_{\tau \sim p_\pi}\!\Big[\sum_{t=0}^{T-1} \gamma^t r(s_t,a_t)\Big] + \alpha H(p_\pi) \;,
\end{equation*}
\]
</p><p>which coincides with the maximum-entropy reinforcement learning objective.
</p>
<p>Equation \(\eqref{eq:kl-divergence}\), the <i>reverse KL divergence</i>, tends to be mode-seeking. Because the expectation is taken under \(p_\pi\), regions where \(p_\pi(\tau)=0\) contribute nothing even if \(p_\mu(\tau)\) is large, while placing mass where \(p_\mu(\tau)\approx 0\) incurs an infinite (or very large) penalty. This asymmetry encourages \(p_\pi\) to concentrate on one (or a few) high-probability modes of \(p_\mu\) rather than spreading mass across all modes. This behavior is consistent with RL in fully observed MDPs with an expected-return objective, where there exists an optimal deterministic policy that concentrates on \(\arg\max_a Q^*(s,a)\) (though stochastic optima can arise with ties, partial observability, or explicit regularization).
</p>
<table class="imgtable"><tr><td>
<img src="static/images/kl-divergence/reverse-kl-divergence.png" alt="reverse formulation of KL divergence" width="400px" />&nbsp;</td>
<td align="left"></td></tr></table>
<table id="caption">
<tr class="r1"><td class="c1">\(p_\mu\) is a bimodal distribution and \(p_\pi\) is a Gaussian approximation. Reverse KL places mass on a single mode of \(p_\mu\).
</td></tr></table>
<h3>Forward KL Divergence</h3>
<p>Reverse KL divergence is natural in policy optimization, since expectations are taken under the policy \(p_\pi\), which can be sampled directly. In supervised learning, by contrast, samples from the data distribution \(p_\mu\) are available, but its density is unknown. A parametric model \(p_\pi\) is therefore introduced to approximate \(p_\mu\), leading to the <i>forward KL divergence</i>, where the expectation in Equation \(\eqref{eq:kl-divergence}\) is taken with respect to \(p_\mu\):
</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:forward-kl}
    D_{KL}(p_\mu \,\Vert\, p_\pi) = \mathbb{E}_{\color{red}{\tau \sim p_\mu}} \left[ \log \frac{p_\mu(\tau)}{p_\pi(\tau)} \right] \;.
\end{equation}
\]
</p><p>Terms with \(p_\mu(\tau)=0\) vanish, so only regions where \(p_\mu\) has support contribute. Forward KL divergence thus penalizes \(p_\pi\) for assigning low probability where \(p_\mu\) is positive, but not for placing mass where \(p_\mu\) is zero. Minimization therefore requires \(p_\pi\) to cover the full support of \(p_\mu\).
</p>
<table class="imgtable"><tr><td>
<img src="static/images/kl-divergence/forward-kl-divergence.png" alt="forward formulation of KL divergence" width="400px" />&nbsp;</td>
<td align="left"></td></tr></table>
<table id="caption">
<tr class="r1"><td class="c1">\(p_\mu\) is a bimodal distribution and \(p_\pi\) is a Gaussian approximation. Because \(p_\pi\) must spread mass across the support of \(p_\mu\), forward KL divergence is characterized as mean-seeking.
</td></tr></table>
<p>Forward KL coincides with maximum likelihood estimation, which provides the statistical foundation for standard supervised learning objectives. Given samples from the data distribution \(p_\mu\), a parametric model \(p_\pi\) is chosen to minimize \(D_{KL}(p_\mu \Vert p_\pi)\):
</p>
<p style="text-align:center">
\[
\begin{align*}
    D_{KL}(p_\mu \Vert p_\pi)
    &amp;= \mathbb{E}_{x \sim p_\mu}\!\left[\log p_\mu(x) - \log p_\pi(x)\right] &amp;&amp; \text{by Equation } \eqref{eq:forward-kl} \\
    &amp;= \mathbb{E}_{x \sim p_\mu}\!\left[\log p_\mu(x)\right] - \mathbb{E}_{x \sim p_\mu}\!\left[\log p_\pi(x)\right] &amp;&amp; \text{linearity of expectation} \\
    &amp;= -\mathbb{E}_{x \sim p_\mu}[\log p_\pi(x)] - H(p_\mu) &amp;&amp; \text{definition of Shannon entropy.}
\end{align*}
\]
</p><p>The entropy term \(H(p_\mu)\) is independent of \(\pi\), so minimizing forward KL divergence is equivalent to maximizing the log-likelihood of the data under \(p_\pi\).  
</p>
<p>For classification, let \(p_\pi(y\mid x)\) be the categorical distribution defined by \(f_\pi(x)\). The cross-entropy loss:
</p>
<p style="text-align:center">
\[
\begin{equation*}
    \mathcal{L}_{\text{CE}} = \mathbb{E}_{(x,y)\sim p_\mu}\!\big[-\log p_\pi(y\mid x)\big]
\end{equation*}
\]
</p><p>differs from \(D_{KL}(p_\mu\Vert p_\pi)\) only by the constant \(H(p_\mu)\). Minimizing cross-entropy is therefore equivalent to minimizing forward KL.  
</p>
<p>For regression, if \(p_\pi(y\mid x)\) is Gaussian with mean \(f_\pi(x)\) and fixed variance, then:
</p>
<p style="text-align:center">
\[
\begin{equation*}
    -\log p_\pi(y\mid x)=\tfrac{1}{2\sigma^2}|y-f_\pi(x)|_2^2 + C
\end{equation*}
\]
</p><p>for fixed variance \(\sigma^2\). Thus, minimizing mean squared error coincides with forward KL under the Gaussian model. Both cross-entropy (classification) and squared error (regression) are therefore special cases of forward KL minimization.
</p>
<h3>References</h3>
<dl>
<dt><a href="https://dibyaghosh.com/blog/probability/kldivergence" target=&ldquo;blank&rdquo;>KL Divergence for Machine Learning</a> (2018)</dt>
<dd><p>Dibya Ghosh
</p></dd>
</dl>
<dl>
<dt><a href="https://jaketae.github.io/study/information-entropy" target=&ldquo;blank&rdquo;>Demystifying Entropy (And More)</a> (2019)</dt>
<dd><p>Jake Tae
</p></dd>
</dl>
<dl>
<dt><a href="https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence" target=&ldquo;blank&rdquo;>Six (and a half) intuitions for KL divergence</a> (2022)</dt>
<dd><p>Callum McDougall
</p></dd>
</dl>
<div id="footer">
<div id="footer-text">
Page generated 2025-11-29 14:59:54 EST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
<!-- GoatCounter Analytics -->
<script data-goatcounter="https://mattlanders.goatcounter.com/count"
        async src="//gc.zgo.at/count.js">
</script>
</head>
<body>
