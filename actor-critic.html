<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Actor-Critic Framework</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Matthew Landers</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="industry-experience.html">Experience</a></div>
<div class="menu-item"><a href="static/Matthew_Landers_CV.pdf" target="blank">CV</a></div>
<div class="menu-category">Notes</div>
<div class="menu-item"><a href="about.html"><i>About&nbsp;these&nbsp;notes</i></a></div>
<div class="menu-item"><a href="glossary.html">Glossary</a></div>
<div class="menu-item"><a href="pseudocode.html">Pseudocode</a></div>
<div class="menu-item"><a href="about-rl.html">About&nbsp;RL</a></div>
<div class="menu-item"><a href="mdp.html">MDPs</a></div>
<div class="menu-item"><a href="value-functions-and-policies.html">Value&nbsp;Func.&nbsp;&amp;&nbsp;Policies</a></div>
<div class="menu-item"><a href="dynamic-programming-for-mdps.html">DP&nbsp;for&nbsp;MDPs</a></div>
<div class="menu-item"><a href="policy-and-value-iteration-proofs.html">DP&nbsp;for&nbsp;MDPs&nbsp;Proofs</a></div>
<div class="menu-item"><a href="model-free-prediction.html">Model-Free&nbsp;Prediction</a></div>
<div class="menu-item"><a href="prediction-with-function-approximation.html">Prediction&nbsp;with&nbsp;Approx.</a></div>
<div class="menu-item"><a href="model-free-control.html">Model-Free&nbsp;Control</a></div>
<div class="menu-item"><a href="on-policy-control-with-function-approximation.html">On-Policy&nbsp;Control<br /> with&nbsp;Approximation</a></div>
<div class="menu-item"><a href="off-policy-control-with-function-approximation.html">Off-Policy&nbsp;Control<br /> with&nbsp;Approximation</a></div>
<div class="menu-item"><a href="importance-sampling.html">Importance&nbsp;Sampling</a></div>
<div class="menu-item"><a href="learnability-of-rl-objectives.html">Learnability&nbsp;of<br /> RL&nbsp;Objectives</a></div>
<div class="menu-item"><a href="the-deadly-triad.html">The&nbsp;Deadly&nbsp;Triad</a></div>
<div class="menu-item"><a href="deep-q-learning.html">Deep&nbsp;Q-Learning</a></div>
<div class="menu-item"><a href="policy-gradients.html">Policy&nbsp;Gradients</a></div>
<div class="menu-item"><a href="actor-critic.html" class="current">Actor-Critic&nbsp;Framework</a></div>
<div class="menu-item"><a href="ddpg.html">DPG&nbsp;&amp;&nbsp;DDPG</a></div>
<div class="menu-item"><a href="reparameterization-trick.html">Reparameterization&nbsp;Trick</a></div>
<div class="menu-item"><a href="sac.html">Soft&nbsp;Actor-Critic</a></div>
<div class="menu-item"><a href="grpo.html">Group&nbsp;Relative&nbsp;Policy<br /> Optimization</a></div>
<div class="menu-item"><a href="transformer.html">Transformer</a></div>
<div class="menu-item"><a href="trpo.html">TRPO</a></div>
<div class="menu-item"><a href="conjugate-gradient-method.html">Conjugate&nbsp;Gradient&nbsp;Method</a></div>
<div class="menu-item"><a href="ppo.html">PPO</a></div>
<div class="menu-item"><a href="double-q-learning.html">Double&nbsp;Q-Learning</a></div>
<div class="menu-item"><a href="td3.html">TD3</a></div>
<div class="menu-item"><a href="nn-verification.html">NN&nbsp;Verification</a></div>
<div class="menu-item"><a href="drl-verification.html">DRL&nbsp;Verification</a></div>
<div class="menu-item"><a href="alphazero.html">AlphaZero&nbsp;(chess)</a></div>
<div class="menu-item"><a href="bayes-theorem-for-probability-distributions.html">Bayes&rsquo;&nbsp;for&nbsp;Distributions</a></div>
<div class="menu-item"><a href="backpropagation.html">Backpropagation</a></div>
<div class="menu-item"><a href="off-policy-evaluation.html">Off-Policy&nbsp;Evaluation</a></div>
<div class="menu-item"><a href="constrained-mdps.html">Constrained&nbsp;MDPs</a></div>
<div class="menu-item"><a href="cpo.html">Constrained&nbsp;Policy&nbsp;<br />Optimization</a></div>
<div class="menu-item"><a href="pid-lagrangian.html">PID&nbsp;Lagrangian</a></div>
<div class="menu-item"><a href="successor-features.html">Successor&nbsp;Features</a></div>
<div class="menu-item"><a href="policy-distillation.html">Policy&nbsp;Distillation</a></div>
<div class="menu-item"><a href="kl-divergence.html">KL&nbsp;Divergence</a></div>
<div class="menu-item"><a href="implicit-q-learning.html">Implicit&nbsp;Q-Learning</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Actor-Critic Framework</h1>
</div>
<p><i>Revised January 2, 2025</i>
</p>
<p><i>The <a href="policy-gradients.html" target=&ldquo;blank&rdquo;>Policy Gradients note</a> is optional but recommended background reading.</i>
</p>
<p>Action-value reinforcement learning methods (e.g., <a href="pseudocode.html" target=&ldquo;blank&rdquo;>Deep Q-learning</a>) select actions by estimating the expected return for each action in a given state. Policy gradient algorithms, by contrast, (e.g., <a href="pseudocode.html" target=&ldquo;blank&rdquo;>REINFORCE</a>) optimize a parameterized policy directly. Actor-Critic methods are a hybrid approach, combining these paradigms by separately approximating the policy (the &ldquo;actor&rdquo;) and the value function (the &ldquo;critic&rdquo;).
</p>
<table class="imgtable"><tr><td>
<img src="static/images/actor-critic/venn-diagram.png" alt="venn diagram of action-value, actor-critic, and policy gradients" width="350px" />&nbsp;</td>
<td align="left"></td></tr></table>
<table id="caption">
<tr class="r1"><td class="c1">Image from <a href="https://www.davidsilver.uk/teaching/" target=&ldquo;blank&rdquo;>Policy Gradient</a>, Lectures on Reinforcement Learning (2015) by David Silver.
</td></tr></table>
<p>Actor-critic methods are designed to address the limitations of standard policy gradient approaches. Consider the update rule for <a href="pseudocode.html" target=&ldquo;blank&rdquo;>REINFORCE</a>, a foundational policy gradient method:
</p>
<p style="text-align:center">
\[
\begin{align}\label{eq:reinforce-update}
    \theta_{t+1} &amp;= \theta_t + \alpha \gamma^t G_t \nabla_\theta \ln \pi(a_t \mid s_t; \theta) \;.
\end{align}
\]
</p><p>Because Equation \(\eqref{eq:reinforce-update}\) relies on the complete return, \(G_t\), REINFORCE is a <a href="glossary.html" target=&ldquo;blank&rdquo;>Monte Carlo</a> (MC) method. Like other MC methods, REINFORCE can exhibit high variance and sample inefficiency due to its reliance on complete trajectories. Actorâ€“Critic methods replace the Monte Carlo return \(G_t\) in the policy-gradient update with an action-value function \(Q(s_t,a_t;\psi)\), which is estimated using <a href="glossary.html" target=&ldquo;blank&rdquo;>temporal-difference</a> (TD) learning. Because TD uses bootstrapped targets instead of full returns, the resulting gradient estimates have lower variance and improved sample efficiency relative to MC methods.
</p>
<p>Actor-critic methods comprise two components: an actor and a critic. The actor selects actions according to the current policy, \(\pi\). The critic evaluates the quality of these actions by computing the <a href="glossary.html" target=&ldquo;blank&rdquo;>TD error</a> or <a href="glossary.html" target=&ldquo;blank&rdquo;>Advantage</a> (using \(V(s)\)), or by estimating \(Q(s,a)\). This evaluation guides the actor in adjusting the policy to achieve better performance. <b>Fundamentally, the actor decides what to do, and the critic evaluates those decisions, providing a scalar learning signal (TD error, advantage, or \(Q\) estimate) that the actor uses to improve the policy.</b>.
</p>
<h3>The Critic</h3>
<p>In practice, both the actor and the critic are implemented as parameterized models, often using neural networks. These models are often governed by distinct parameter sets, \(\theta\) for the actor and \(\psi\) for the critic.
</p>
<p>Because the critic estimates a value function, its parameters, \(\psi\), are updated to minimize the squared TD error:
</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:td-error}
    \delta_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}; \psi) - Q(s_t, a_t; \psi) \;,
\end{equation}
\]
</p><p>This error yields the following update rule:
</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:critic-update}
    \psi \leftarrow \psi + \alpha^\psi \delta_t \nabla_{\psi} Q(s_t, a_t; \psi) \;.
\end{equation}
\]
</p><p>Importantly, while action-value methods like <a href="model-free-control.html" target=&ldquo;blank&rdquo;>Q-learning</a> and its derivatives, such as <a href="deep-q-learning.html" target=&ldquo;blank&rdquo;>deep Q-learning</a>, directly learn the \(Q\)-function associated with the optimal policy, actor-critic methods learn the \(Q\)-function corresponding to the actor's policy, \(\pi(a \mid s; \theta)\). Although the actor does not directly appear in the critic's update rule, it indirectly influences the critic's updates because the actions \(a_t\) and \(a_{t+1}\) are sampled from the actor's policy, \(\pi(a \mid s; \theta)\).
</p>
<p>Note that while the <a href="glossary.html" target=&ldquo;blank&rdquo;>action-value function</a> is used in Equations \(\eqref{eq:td-error}\) and \(\eqref{eq:critic-update}\), the <a href="glossary.html" target=&ldquo;blank&rdquo;>state-value function</a> is a valid and commonly used alternative.
</p>
<h3>The Actor</h3>
<p>Because the actor learns a parameterized policy, its loss function is derived from the <a href="policy-gradients.html" target=&ldquo;blank&rdquo;>policy gradient theorem</a>:
</p>
<p style="text-align:center">
\[
\begin{align}
    \nabla_\theta J(\theta) &amp;\propto \sum_{s \in \mathcal{S}} \mu(s) \sum_{a \in \mathcal{A}} Q_\pi(s, a) \nabla_\theta \pi(a \mid s; \theta) \nonumber \\
    &amp;= \mathbb{E}_{s \sim \mu, a \sim \pi(\cdot \mid s; \theta)} \left[ Q_\pi(s_t, a_t) \nabla_\theta \ln \pi(a_t \mid s_t; \theta) \right] \;. \label{eq:pgt}
\end{align}
\]
</p><p>Unlike pure policy gradient methods, however, actor-critic methods do not compute the true policy gradient. Instead, they approximate it using the value function estimated by the critic, which guides the agent toward policies the critic predicts will yield higher rewards. Concretely, the true value function in Equation \(\eqref{eq:pgt}\) is replaced by the critic's estimate, yielding:
</p>
<p style="text-align:center">
\[
\begin{equation*}
    \nabla_{\theta} J(\theta) \approx \mathbb{E}_{s \sim \mu, a \sim \pi(\cdot \mid s; \theta)} \left[ Q(s, a; \psi) \nabla_{\theta} \ln \pi(a \mid s; \theta) \right] \;,
\end{equation*}
\]
</p><p>where \(Q(s, a; \psi)\) is the value provided by the critic.
</p>
<p>Note that the critic does not need to estimate the action-value function \(Q(s, a; \psi)\) as in Equations \(\eqref{eq:td-error}\) and \(\eqref{eq:pgt}\). It could be \(\delta_t\) or other critic-derived signals (e.g., advantages) as long as they provide a suitable surrogate for \(Q_\pi\) or the advantage and yield a (possibly biased but useful) gradient estimator. Indeed, many of the most popular actor-critic methods, such as <a href="trpo.html" target=&ldquo;blank&rdquo;>TRPO</a> and <a href="ppo.html" target=&ldquo;blank&rdquo;>PPO</a>, use a variety of objectives. For example, one-step actor-critic, the policy gradient analog of TD(0), uses the state-value function \(V(s; \psi)\) to compute TD error:
</p>
<table class="imgtable"><tr><td>
<img src="static/images/actor-critic/one-step-actor-critic.png" alt="one step actor critic pseudocode" width="600px" />&nbsp;</td>
<td align="left"></td></tr></table>
<h3>References</h3>
<dl>
<dt><a href="http://incompleteideas.net/book/the-book-2nd.html" target=&ldquo;blank&rdquo;>Reinforcement Learning: An Introduction</a>
(2018)</dt>
<dd><p>Richard S. Sutton and Andrew G. Barto
</p></dd>
</dl>
<dl>
<dt><a href="https://www.davidsilver.uk/teaching/" target=&ldquo;blank&rdquo;>Policy Gradient</a>, Lectures on Reinforcement Learning (2015)</dt>
<dd><p>David Silver
</p></dd>
</dl>
<dl>
<dt><a href="https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf" target=&ldquo;blank&rdquo;>Actor-Critic
Algorithms</a>, Advances in neural information processing systems (2000)</dt>
<dd><p>Vijay R. Konda and John N. Tsitsiklis
</p></dd>
</dl>
<div id="footer">
<div id="footer-text">
Page generated 2025-12-04 15:37:28 PST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
<!-- GoatCounter Analytics -->
<script data-goatcounter="https://mattlanders.goatcounter.com/count"
        async src="//gc.zgo.at/count.js">
</script>
</head>
<body>
