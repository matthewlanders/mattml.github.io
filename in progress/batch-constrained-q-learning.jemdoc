# jemdoc: menu{MENU}{batch-constrained-q-learning.html}
= Batch-Constrained Q-learning

In many real-world settings traditional online RL is not feasible due to safety, monetary cost, or ethics. In these
cases, offline RL must be used. The objective in offline RL is the same as typical RL — we want to find the policy that
maximizes expected cumulative discounted reward from the set of constraint-respecting policies. However, the value of
the optimal policy must be learned from a fixed dataset of the system’s behavior $\mathcal{B} = \{(s_t,a_t,r_t,s_{t+1})^i\}_{i=0}^N$.
In healthcare, for example — where conventional active reinforcement learning is typically prohibitively dangerous —
the offline dataset comprises treatment histories of real patients. This dataset may be generated by a single policy or
by a mixture of policies collectively referred to as the behavior policy $\pi_{\mathcal{B}}$. The policy or policies that
constitute $\pi_{\mathcal{B}}$ may be optimal or suboptimal. Importantly, regardless of  $\pi_{\mathcal{B}}$,
off-policy algorithms can fail in the offline setting. In fact, it [https://arxiv.org/pdf/1812.02900.pdf has been shown]
that off-policy agents perform dramatically worse than the behavioral agent even when trained with the same algorithm
on the same dataset. This is due to a phenomenon called extrapolation error — unseen state-action pairs are erroneously
estimated to have unrealistic values.

To mitigate extrapolation error policy constraint methods ensure that the distribution over actions under which we
compute Q-values $\pi(a \mid s)$ is ``close'' to the behavior distribution $\pi_{\mathcal{B}}(a \mid s)$. One approach —
[https://arxiv.org/pdf/1812.02900.pdf Batch-Constrained deep Q-learning (BCQ)] — takes a state and generates candidate
actions with high ``similarity'' to the dataset. It then selects the highest valued action via a learned Q-network. In this
way, BCQ learns a policy with a similar state-action visitation to the data in the batch. To measure the similarity of a
state-action pair to those present in $\mathcal{B}$, the approach uses a learned state-conditioned marginal likelihood
$P_{\mathcal{B}}(a \mid s)$.

=== State-conditioned marginal likelihood

The marginal likelihood is a likelihood function — the joint probability of the observed data given the parameters
$\phi$ of the statistical model — that has been integrated over the parameter space $\Phi$:

\(
\begin{equation*}
    m(x) = \int_\Phi p(x \mid \phi) p(\phi) \; \text{d}\phi \; ,
\end{equation*}
\)

where where $x$ is the observed data, $\phi$ is the parameter vector, $p(x \mid \phi)$ is the likelihood function
and $p(\phi)$ is the prior distribution; a probability distribution that reflects our belief about the parameter
value before we observe $x$. In Bayesian statistics the marginal likelihood gives the probability of the observed data
under a given model and prior distribution. That is, it gives the probability of the observed data while accounting for
uncertainty in the model parameters ($\phi$ has been integrated out or [glossary.html /marginalized/]).

In BCQ we use the state-conditioned marginal likelihood to find the probability of taking an action in a particular
state. First let us examine how we can use the marginal likelihood to find the probability of taking an action
irrespective of the state:

\(
\begin{equation*}
    P_{\mathcal{B}}(a) = \int_\Phi p(a \mid \phi) p(\phi) \; \text{d}\phi \; ,
\end{equation*}
\)

where $p(a \mid \phi)$ is the sampling density, which describes the probability of taking $a$ given the model
parameters $\phi$, and $p(\phi)$ is the prior distribution of the model parameters $\phi$, which describes our
prior uncertainty about the parameters before observing $a$.

The marginal likelihood measures the goodness of fit of a model to the data, averaged over all possible values of the
model parameters $\phi$. Here we are interested in the probability of taking action $a$ so $\phi$ parameterizes a
model of the distribution over actions. We might have some initial estimate for $\phi$ — we can (optionally) use
$\mathcal{B}$ to set $\phi$, for example. This is referred to as our prior belief. There is, however, a
fundamental uncertainty about the value of $\phi$ (even if we use $\mathcal{B}$ to define our prior it is impossible to
sample exhaustively for a high-dimensional continuous action space). By integrating over all possible values of $\phi$,
we account for this uncertainty. That is, we can interpret the integral $P_{\mathcal{B}}(a)$ as a weighted average over
all possible values of $\phi$ — the expected probability of taking $a$ over all possible parameters $\phi \in \Phi$,
weighted by the prior.

This intuition holds for the /state-conditioned marginal likelihood/. All we must do is condition the selection of the
action $a$ on a particular state $s$:

\(
\begin{equation}\label{eq:state-conditioned-marginal-likelihood}
    P_{\mathcal{B}}(a \mid s) = \int_\Phi p(a \mid s, \phi) p(\phi \mid s) \; \text{d}\phi \; ,
\end{equation}
\)

where $p(a \mid s, \phi)$ is the sampling density, which describes the probability of taking $a$ given the state $s$
and the model parameters $\phi$, and $p(\phi \mid s)$ is the prior distribution of the model parameters $\phi$,
which describes our prior uncertainty about the parameters before observing $a$, given $s$. For additional background,
see the [bayes-theorem-for-probability-distributions.html Bayes’ Theorem for probability distributions note].

Because the state-conditioned marginal likelihood $P_{\mathcal{B}}(a \mid s)$ gives the weighted probability of
taking $a$ given $s$ under $\pi_{\mathcal{B}}$, the policy that maximizes $P_{\mathcal{B}}(a \mid s)$ also minimizes
the extrapolation error induced by distant or unseen state-action pairs.

There is, however, an issue. Because we need many parameters to model the probability distribution over actions, when
action spaces are multi-dimensional and continuous solving $\eqref{eq:state-conditioned-marginal-likelihood}$ requires a
high-dimensional integration which can be intractable. Thus, to reasonably approximate
$\text{argmax}_a P_{\mathcal{B}}(a \mid s)$, BCQ uses a parametric generative model of the batch $G_\omega(s)$ from
which actions can be sampled. More specifically, $n$ actions are sampled from $G_\omega(s)$ and the action with the
highest value (as estimated by the value function $Q_\theta$) is selected.

For its generative model, BCQ uses a conditional variational auto-encoder (VAE).

- Given a state, we want to come up with actions to select from
- Assume that, for a particular state, we are given a set of candidate actions from which to select
$\mathcal{A}_\mathcal{D}$
- Typically we would select $\text{argmax}_a q(s,a)$
- In the offline setting, however, we need to avoid extrapolation error. Thus, we need the the state-action pair to be
both high in value and be similar to those in $\mathcal{D}$
- We can assess the similarity of a state-action pair to those in $\mathcal{D}$ using state-conditioned marginal
likelihood, which tells us the probability that $a$ was taken in $s$ under $\pi_\mathcal{B}$
- However, solving the state-conditioned marginal likelihood is intractable when modeling the action space requires many
parameters (such is the case when the action space is multi-dimensional and continuous)
- To resolve this, we use a generative model that probabilistically proposes a set of candidate actions
- The likelihood that any action is in the proposed set is based on the likelihood that it would be taken in $s$ under $\pi_\mathcal{B}$
- This, of course, is what marginal likelihood tells us. Having just described that this calculation is intractable it
is reasonable to wonder how the generative model can determine the likelihood that $a$ is taken in $s$ under $\pi_\mathcal{B}$
- A detailed description of the mechanisms underlying this computation is included in the [variational-autoencoder.html
variational autoencoder note] however, in general terms BCQ uses a VAE as follows:

Let $x \in \mathbb{R}^{\lvert s \rvert \times \lvert a \rvert}$ be the state-action pair input to a neural network
called an encoder. The encoder reduces the input's dimension to a specified size (via a hyperparameter) $n_e$ and
returns this vector $\mathbb{R}^{n_e}$ as an output. The values in this vector represent the parameters that define a
probability distribution over actions in the latent space (states are ignored by the encoder). That is, we
do not know the true representation of the input $x$ in the latent space $z$ so we model the possible latent
representations as a probability distribution. During training, by improving the parameters the VAE learns to associate
the input vector $x$ to a region in the latent space where its lower dimensional representation $z$ is most likely to
be. We typically assume this distribution to be [https://en.wikipedia.org/wiki/Multivariate_normal_distribution
multivariate Gaussian]. In this case the parameters in $z$ are the the mean $\mu$ and variance $\sigma$ of the
distribution. Using this distribution, we sample a point $z \in \mathbb{R}^{n_e}$ from the latent space. Notice that
this makes a VAE probabilistic. The latent vector $z$ and the state $s$ are then passed as input to a decoder network
which attempts to reconstruct the input $\hat{x} \in \mathbb{R}^{\lvert s \rvert \times \lvert a \rvert}$. Much like
the encoder, the decoder does not attempt to reconstruct $x$ directly. Instead, it outputs parameters for a (typically
multivariate Gaussian) conditional distribution of the observation. Unlike the encoder, however, the decoder typically
only outputs the mean parameters. The variance is assumed to be fixed (or can be learned independently). This practical
because learning is easier if we only have to optimize the mean parameters.

From this description, it does not appear that the state is used by either the encoder or decoder. It does, however,
serve an important purpose — it restricts the latent action space representation. Specifically, BCQ uses a /conditional
VAE/ (CVAE). In a standard VAE we have no way of constraining the data generating process. For example, if we had a
VAE trained to generated handwritten digits there is no mechanism by which we can generate a specific number. In BCQ
this limitation restricts us from generating an action for a specific state. In a CVAE, by contrast, we can condition
the encoder and decoder on an additional variable $c$ (or variables). This requires a change in the
[variational-autoencoder.html\#mjx-eqn-eq%3Aelbo-2 standard VAE loss]:

\(
\begin{equation*}
    \text{ELBO}(q) = \mathbb{E}_{z \sim Q} \big[ \log p(x \mid z) \big] - D_{KL}(Q(Z \mid x) || P(Z))
\end{equation*}
\)

to:

\(
\begin{equation*}
    \text{ELBO}(q) = \mathbb{E}_{z \sim Q} \big[ \log p(x \mid z,c) \big] - D_{KL}(Q(Z \mid x,c) || P(Z \mid c)) \; .
\end{equation*}
\)

=== Learning a policy

The generative model $G_\omega$ is used in conjunction with the standard action-value function $Q_\theta$ to select
actions. Specifically, for a given state $s$, $n$ actions are sampled from $G_\omega$. BCQ then selects the action for
which the state-value function is the highest $Q_\theta(s,a_i)$. To diversify the set of actions, BCQ uses a
perturbation model $\xi_\phi(s,a,\Phi)$ which outputs an adjustment to the action $a$ in the range $[-\Phi,\Phi]$.
As an alternative, we could sample actions from the generative model until a diverse set were considered, however, this
might require a prohibitive number of samples. In summary actions are selected:

\(
\begin{align*}
    \pi(s) = \text{argmax}_{a_i + \xi_\phi(s,a_i,\Phi)} Q_\theta \big( s,a_i + \xi_\phi(s,a_i,\Phi) \big) \; , &&
    \{a_i \sim G_\omega(s) \}_{i=1}^n \; .
\end{align*}
\)

Both $n$ and $\Phi$ are hyperparameters, the value of which that place BCQ on the spectrum from imitation learning to
reinforcement learning. If $n=1$ and $\Phi=0$ then the policy resembles behavioral cloning. As $n \rightarrow \infty$
and $\Phi \rightarrow (a_\text{max} - a_\text{min})$, then BCQ approaches Q-learning because the policy begins to
greedily maximize the value function over the entire action space.

==== The perturbation model

The parameterized perturbation model $\xi_\phi$ can be trained to maximize $Q_\theta(s,a)$ with a
[http://proceedings.mlr.press/v32/silver14.pdf deterministic policy gradient algorithm] by sampling $a \sim G_\omega(s)$:

\(
\begin{equation*}
    \phi \gets \text{argmax}_\phi \sum_{(s,a) \in \mathcal{D}} Q_\theta \big( s,a_i + \xi_\phi(s,a_i,\Phi) \big) \; .
\end{equation*}
\)

==== The action-value function

BCQ estimates the action-value function using [https://arxiv.org/pdf/1802.09477.pdf clipped double Q-learning].


#The min_{j=1,2} Q_j means it uses the Q-value from the Q network (Q_1 or Q_2) which estimates the lower Q-value. max_{j=1,2} Q_j means it uses the Q-value from the Q network (Q_1 or Q_2) which estimates the higher q-value.


TODO:
- how is learning performed
- finish continuous description
- discrete
- capital vs lowercase Z
(or is this for VAEs?)

=== References

: {[https://arxiv.org/pdf/1812.02900.pdf Off-Policy Deep Reinforcement Learning without Exploration], International
Conference on Machine Learning (2019)} Scott Fujimoto, David Meger, and Doina Precup

: {[https://arxiv.org/pdf/1312.6114.pdf Auto-Encoding Variational Bayes] (2013)} Diederik P Kingma and Max Welling

: {[https://arxiv.org/pdf/1906.02691.pdf An Introduction to Variational Autoencoders], Foundations and
Trends in Machine Learning (2019)} Diederik P Kingma and Max Welling

: {[https://arxiv.org/pdf/2005.01643.pdf Offline Reinforcement Learning- Tutorial, Review, and Perspectives on Open
Problems] (2020)} Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu

: {[https://papers.nips.cc/paper_files/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf Learning Structured
Output Representation using Deep Conditional Generative Models], Advances in neural information processing systems
(2015)} Kihyuk Sohn, Honglak Lee, and Xinchen Yan

: {[https://papers.nips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf Double Q-learning]
Advances in neural information processing systems (2010)} Hado van Hasselt

: {[https://arxiv.org/pdf/1509.06461.pdf Deep Reinforcement Learning with Double Q-learning], Proceedings of the
AAAI conference on artificial intelligence (2016)} Hado van Hasselt, Arthur Guez, and David Silver

: {[https://arxiv.org/pdf/1802.09477.pdf Addressing Function Approximation Error in Actor-Critic Methods],
International conference on machine learning (2018} Scott Fujimoto, Herke van Hoof, and David Meger
