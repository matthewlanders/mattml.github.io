# jemdoc: menu{MENU}{meta-reinforcement-learning.html}
= Meta Reinforcement Learning

Meta-reinforcement learning (meta-RL) is a machine learning subfield in which methods are designed to /learn to reinforcement
learn/. That is, where traditional RL learns a policy, meta-RL learns a function $f(\cdot; \theta)$ parameterized by /meta-parameters/
$\theta$ that — given a /meta-trajectory/ $\mathcal{D}$ which may contain multiple policy episodes — outputs the parameters a policy
$\pi_\phi$ directly: $\phi = f(\mathcal{D};\theta)$. In so doing, meta-RL can address RL's sample inefficiency problem
at test time and can even solve previously intractable problems. For example, in /few-shot meta-RL/ approaches are designed
to quickly adapt to new tasks using only a few environmental interactions.

To accomplish this, $f(\cdot; \theta)$ is trained during a meta-learning phase on a set of /tasks/ which come from a distribution
$p(\mathcal{M})$ (in theory any set of tasks may comprise $\mathcal{M}$ but in practice $\mathcal{S}$ and $\mathcal{A}$
are typically held constant across all tasks while the reward function, dynamics, and\/or initial state  distributions
differ). This implies a two-tiered learning structure — in the /outer-loop/ we perform /meta-training/ and in the
/inner-loop/ we perform /adaptation/. Meta-training is designed to improve the efficiency with which an agent can learn
a new task by using its experiences over many tasks to gradually adjust the parameters $\phi$ that govern the agent's
action in the inner loop. During adaption we learn the parameters $\phi$ that optimize the policy $\pi_\phi$ for
a specific task $\mathcal{M^i}$ sampled from $\mathcal{M}$. The agent's interaction with $\mathcal{M^i}$ in the
inner-loop is called a /lifetime/ or /trial/.

Our goal is meta-train an inner-loop that can adapt quickly to a new MDP. We measure the performance of a meta-RL
algorithm by the returns earned by $\pi_\phi$; i.e., the policy produced during adaptation. More formally:

\(
\begin{equation*}
    J(\theta) = \mathbb{E}_{\mathcal{M}^i \sim p(\mathcal{M})} \Bigg[ \mathbb{E}_{\mathcal{D}} \Bigg[
    \sum_{\tau \in \mathcal{D}_{K:H}} g(\tau) \mid \theta, \mathcal{M}^i \Bigg] \Bigg] \;,
\end{equation*}
\)

where $\tau$ are trajectories produced by policy $\pi_\phi$ in $\mathcal{M}^i$, $g(\tau)$ is the discounted return,
$H$ is the length of a trial, and $K$ is the length of the /burn-in/ or adaptation period. A burn-in period is a set of
time steps during which the performance of the policies produced by the inner-loop is not important. Note not all
use cases allow for a burn-in period (e.g., due to safety considerations or resource constraints). For such applications
$K=0$.

=== Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (MAML)

!!!!!! https://medium.com/instadeep/model-agnostic-meta-learning-made-simple-3c170881c71a

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (MAML) is a parameterized policy gradient meta-RL
method. That is, it attempts to find parameters $\theta$ that are /sensitive/ to changes in the task, such that small
changes in the parameters will produce large improvements on the loss function of any MDP drawn from $p(\mathcal{M})$
when updated in the direction of the gradient of that loss. Its inner-loop is a policy gradient algorithm whose initial
parameters are the meta-parameters $\phi = \theta$. By defining adaptation in this manner, the inner-loop becomes a
differentiable function of the initial parameters. This means the initialization $\theta$ can be optimized with gradient
descent.

Learning proceeds as follows. We define an initial set of outer-loop parameters $\theta$ and assign the inner loop
parameters for all tasks as $\phi_0 = \theta$. We then update the parameters  $\phi_0^i$ for task $i$ using a
[policy-gradients.html policy gradient method]:

\(
\begin{equation*}
    \phi_1^i = f(\mathcal{D}_s^i;\phi_0) = \phi_0 + \alpha \nabla_{\phi_0} J^I(\phi_0) \;,
\end{equation*}
\)

where $\alpha$ is a learning rate, $J^I(\phi)$ is the standard [policy-gradients.html policy gradient objective] (i.e.,
estimated return), and $\mathcal{D}_s^i$ is the /support set/ — a small set of trajectories generated by $\pi_{\phi_0}$.

The outer- or /meta-objective/ is to find the parameters that optimize the performance of $f(\cdot;\phi_1^i)$ with
respect to $\phi_0$ across tasks sampled from $p(\mathcal{M})$:

\(
\begin{equation*}
    \max_\phi \sum_{\mathcal{M}^i \sim p(\mathcal{M})} J^I(\phi_1^i) \;.
\end{equation*}
\)

Thus, to update the initial parameters $\phi_0$ in the outer-loop, we compute the gradient of the returns of the
updated policy $\pi_{\phi_1^i}$ with respect to the initial parameters:

\(
\begin{equation}\label{eq:outer-loop-update}
    \phi_0 = \phi_0 + \beta \nabla_{\phi_0} \sum_{\mathcal{M}^i \sim p(\mathcal{M})} J^I(\phi_1^i) \;,
\end{equation}
\)

where $\beta$ is a learning rate, $J^I(\phi_1^i)$ is the estimated return for policy $\pi_{\phi_1^i}$ which is evaluated
using the /query set/  $\mathcal{D}_q^i$ and $\nabla_{\phi_0} J^I(\phi_1^i)$ is the gradient of the returns of the
updated policy computed w.r.t. the initial policy parameters $\phi_0$.

~~~
{}{img_left}{static/images/meta-rl/maml.png}{model agnostic meta learning for RL pseudocode}{600px}
~~~

==== Calculating the outer-gradient

Let us consider the outer-loop update $\eqref{eq:outer-loop-update}$ more carefully. To simplify, we will assume that
there is only one task in $\mathcal{M}$: $\phi_1 = f(\mathcal{D}_s; \phi_0) = \phi_0 + \alpha \nabla_{\phi_0} J^I(\phi_0)$.
Then, by the chain rule, we have:

\(
\begin{align}
    \nabla_{\phi_0} J^I(\phi_1) &= \nabla_{\phi_0} J^I\big(f(\mathcal{D}_s;\phi_0)\big) \nonumber \\
    &= \frac{\text{d}\, J^I\big(f(\mathcal{D}_s;\phi_0)\big)}{\text{d}\, \phi_0} \nonumber \\
    &= \frac{\text{d}\, J^I\big(f(\mathcal{D}_s;\phi_0)\big)}{\text{d}\, f(\mathcal{D}_s;\phi_0)} \cdot \frac{\text{d}\, f(\mathcal{D}_s;\phi_0)}{\text{d}\, \phi_0} \nonumber \\
    &= \nabla_{f(\mathcal{D}_s;\phi_0)} J^I\big(f(\mathcal{D}_s;\phi_0)\big) \cdot \nabla_{\phi_0} f(\mathcal{D}_s;\phi_0) \label{eq:outer-update}
\end{align}
\)

The first term $\nabla_{f(\mathcal{D}_s;\phi_0)} J^I\big(f(\mathcal{D}_s;\phi_0)\big)$ is the gradient of the [policy-gradients.html
policy objective function] at the optimized parameter and the second term $\nabla_{\phi_0} f(\mathcal{D}_s;\phi_0) =
\nabla_{\phi_0} \phi_1$ is the Jacobian of the optimization algorithm $f$; it describes how a change in $\phi_0$ is
locally transformed into a change of $\phi_1$. Notice that this term requires a second-order derivatives:

\(
\begin{align*}
    \nabla_{\phi_0} f(\mathcal{D}_s;\phi_0) &= \nabla_{\phi_0} \big( \phi_0 + \alpha \nabla_{\phi_0} J^I(\phi_0) \big) \\
    &= \mathbf{I} + \alpha \nabla_{\phi_0}^2 J^I(\phi_0) \\
    &= \mathbf{I} + \alpha \nabla_{\phi_0} \mathbb{E}_{s,a \sim \pi_{\phi_0}} [ q_\pi(s,a) \nabla_{\phi_0} \ln \pi(a \mid s; \phi_0)]
    && \text{see policy gradient note}
\end{align*}
\)

where $\mathbf{I}$ is the identity matrix, $\nabla_{\phi_0}^2$ denotes a second-order derivative, and $\tau$ is a
trajectory.

~~~
*The Jacobian*

https://lilianweng.github.io/posts/2022-09-08-ntk/
https://mml-book.github.io/book/mml-book_printed.pdf (page 152)
https://stats.stackexchange.com/questions/452556/why-does-the-computation-for-maml-only-improve-by-33-when-removing-higher-order
~~~

Calculating second-order derivatives is computational expensive even when we use a single inner gradient step. In
practice it is often useful to take $m > 1$ inner gradient steps. In such a case, for each $i$ we have:

\(
\begin{align}
    \phi_0^i &= \theta \nonumber \\
    \phi_1^i &= \phi_0^i +\alpha \nabla_{\phi_0} J^I(\phi_0^i) \nonumber \\
    \phi_2^i &= \phi_1^i + \alpha \nabla_{\phi_0} J^I(\phi_1^i) \nonumber \\
    &\dots \nonumber \\
    \phi_m^i &= \phi_{m-1}^i + \alpha \nabla_{\phi_0} J^I(\phi_{m-1}^i) \; \label{eq:multi-step-gradient}
\end{align}
\)

Then we calculate the Jacobian as:

\(
\begin{align*}
    \nabla_{\phi_0} \phi_m &= \prod_{n=1}^{m} \nabla_{\phi_{n-1}} \phi_n \\
    &= \prod_{n=1}^{m} \nabla_{\phi_{n-1}} \phi_{n-1} + \alpha \nabla_{\phi_0} J^I(\phi_{n-1}) && \text{cf. } \eqref{eq:multi-step-gradient} \\
    &= \prod^{m}_{n=1} \left(\mathbf{I} + \alpha \nabla_{\phi_{n-1}}^2 J^I(\phi_{n-1}) \right).
\end{align*}
\)

Consider, for example, when $m=3$ then we have:

\(
\begin{align*}
    \nabla_{\phi_0} J^I(\phi_3) &= \nabla_{\phi_0} J^I\big(f(\mathcal{D}_s;\phi_2)\big) \\
    &= \nabla_{\phi_0} J^I\big(f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_1))\big) \\
    &= \nabla_{\phi_0} J^I\big(f(\mathcal{D}_s;f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0)))\big) \\
    &= J^I\big(f(\mathcal{D}_s;f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0)))\big)^\prime \cdot
    f(\mathcal{D}_s;f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0)))^\prime \cdot f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0))^\prime \cdot
    f(\mathcal{D}_s;\phi_0)^\prime \\

    &= \frac{\text{d}\, J^I\big(f(\mathcal{D}_s;f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0)))\big)}{\text{d}\, f(\mathcal{D}_s;f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0)))}
    \cdot \frac{\text{d}\, f(\mathcal{D}_s;f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0)))}{\text{d}\, f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0))}
    \cdot \frac{\text{d}\, f(\mathcal{D}_s;f(\mathcal{D}_s;\phi_0))}{\text{d}\, f(\mathcal{D}_s;\phi_0)}
    \cdot \frac{\text{d}\, f(\mathcal{D}_s;\phi_0)}{\text{d}\, \phi_0} \\

    &= \nabla_{\phi_3}J^I(\phi_3) \cdot \color{red}{\nabla_{\phi_2} \phi_3 \cdot \nabla_{\phi_1} \phi_2 \cdot \nabla_{\phi_0} \phi_1} \;,
\end{align*}
\)

where the terms in $\color{red}{\text{red}}$ comprise the Jacobian calculation.

While we could calculate the Jacobian for every update, this would be very expensive in terms of both memory and
computation. Notice from $\eqref{eq:outer-update}$, however, that we do not need the Jacobian directly. Instead, we
really need a product of the Jacobian (denoted in $\color{red}{\text{red}}$) with a gradient vector:

\(
\begin{equation*}
    \nabla_{f(\mathcal{D}_s;\phi_0)} J^I\big(f(\mathcal{D}_s;\phi_0)\big) \cdot \color{red}{\nabla_{\phi_0} f(\mathcal{D}_s;\phi_0)} \;.
\end{equation*}
\)

We can exploit this using a ``stop gradient'' — a function $s$ that returns its input and has the following property:

\(
\begin{equation*}
    \frac{\text{d}\, s(f(\cdot))}{\text{d}\,x} = 0\;.
\end{equation*}
\)

That is, $s$ prevents any gradient from flowing through $f$ with, so it is effectively a constant with respect to
$\theta$.

We can apply the stop gradient as follows:

\(
\begin{align}
    \nabla_{\phi_0} J^I(\phi_1) &= \nabla_{f(\mathcal{D}_s;\phi_0)} J^I\big(f(\mathcal{D}_s;\phi_0)\big) \cdot \nabla_{\phi_0} f(\mathcal{D}_s;\phi_0) \nonumber \\
    &= \frac{\text{d}\, \phi_1}{\text{d}\, \phi_0} \cdot \nabla_{\phi_1} J^I(\phi_1) && \text{note the order of the terms of flipped for notational convenience}  \nonumber \\
    &= \frac{\text{d}\, \phi_1}{\text{d}\, \phi_0} \cdot s\big( \nabla_{\phi_1} J^I(\phi_1) \big) \nonumber \\

    &= \frac{\text{d}\, \phi_1}{\text{d}\, \phi_0} \cdot s\big(\nabla_{\phi_1} J^I(\phi_1) \big)
    + \color{red}{\left( \frac{\text{d}\, s\big(\nabla_{\phi_1} J^I(\phi_1) \big)}{\text{d}\, \phi_0} \right) \phi_1} \label{eq:stop-grad-1} \\

    &= \Big( \phi_1 \cdot s\big(\nabla_{\phi_1} J^I(\phi_1) \big) \Big) \; \frac{\text{d}}{\text{d}\, \phi_0}
    && \text{by the product rule} \nonumber
\end{align}
\)

We first introduce the stop gradient in $\eqref{eq:stop-grad-1}$. We can include the $\color{red}{\text{red}}$ terms
because the stop gradient term evaluates to $0$ (as per the definition of a stop gradient).

==== First-Order MAML

=== Fast Reinforcement Learning via Slow Reinforcement Learning

https://www.youtube.com/watch?v=Ho1HrnSkhGE&ab_channel=IEEERoboticsandAutomationSociety (starting around 8:30)
https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
https://lucaslingle.wordpress.com/2021/10/07/on-memory-based-meta-reinforcement-learning/

~~~
{}{img_left}{static/images/meta-rl/rl2.png}{fast RL via slow RL pseudocode}{600px}
~~~

=== References

: {[https://arxiv.org/abs/2301.08028 A Survey of Meta-Reinforcement Learning] (2023)} Jacob Beck, Risto Vuorio,
Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson

: {[https://arxiv.org/pdf/1703.03400.pdf Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks],
International conference on machine learning (2017)} Chelsea Finn, Pieter Abbeel, and Sergey Levine

: {[https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0 Reinforcement learning, fast and slow], Trends in
cognitive sciences (2019)} Matthew Botvinick, Sam Ritter, Jane X. Wang, Zeb Kurth-Nelson, Charles Blundell, and
Demis Hassabis

: {[https://interactive-maml.github.io/ An Interactive Introduction to Model-Agnostic Meta-Learning] (2021)} Luis Müller,
Max Ploner, Thomas Goerttler, and Klaus Obermayer

: {[https://lilianweng.github.io/posts/2019-06-23-meta-rl/ Meta Reinforcement Learning] (2019)} Lilian Weng

: {[https://arxiv.org/pdf/1611.05763.pdf Learning to Reinforcement Learn] (2016)} Jane X Wang, Zeb Kurth-Nelson,
Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick

:{[https://arxiv.org/pdf/1611.02779.pdf RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning] (2016)}
Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel
